{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary libraries for prediciton\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_matrix(matrix, row_label, col_label):\n",
    "    \"\"\"Pretty print of the given matrix \"\"\"\n",
    "\n",
    "    # Restraining labels that are too big\n",
    "    row_label = [el[:10] + '..' if len(el) > 10 else el\n",
    "                for el in row_label]\n",
    "    col_label = [el[:10] + '..' if len(el) > 10 else el\n",
    "                for el in col_label]\n",
    "\n",
    "    # Stringfying everything & Joining top label\n",
    "    s_matrix = [list([\" \"] + (col_label))] + \\\n",
    "               [[row_label[row_idx]] + \\\n",
    "                [str(e) for e in row] for row_idx, row in enumerate(matrix)]\n",
    "\n",
    "    # Length of each matrix column\n",
    "    len_s = [max(map(len, col)) for col in zip(*s_matrix)]\n",
    "\n",
    "    # Cell formatation\n",
    "    formatation = '\\t'.join('{{:{}}}'.format(x) for x in len_s)\n",
    "\n",
    "    # Apply cell formation to each matrix element\n",
    "    pretty_mat = [formatation.format(*row) for row in s_matrix]\n",
    "\n",
    "    # Print Pretty Matrix\n",
    "    print('\\n'.join(pretty_mat))\n",
    "\n",
    "\n",
    "def display_confusion_matrix(values):\n",
    "    '''Display the given array as a confusion matrix'''\n",
    "    pretty_matrix([values[0:2], values[2:4]],\n",
    "                  ['Actual NO', 'Actual YES'],\n",
    "                  ['Predic NO', 'Predic YES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Macros\n",
    "K_FOLD_NUM_SPLITS = 5\n",
    "SEED = 42\n",
    "\n",
    "# Pretty printer\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Algorithms\n",
    "\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DT():\n",
    "    '''Create a new Decision Tree'''\n",
    "    # Useful DecisionTree tutorial:\n",
    "    # https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "    return DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RF():\n",
    "    '''Create a new Ranfom Forest model'''\n",
    "    return RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_GB():\n",
    "    '''Create a new Gradient Boosting model'''\n",
    "    return GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "* Predictions are done in this notebook.\n",
    "* It is also useful to compare how serveral algorithms perform against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  utils.read_csv_to_df('dataset/preprocessed_data.csv')\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_COL = dataset.columns.get_loc(\"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and Y\n",
    "X = dataset.iloc[:, 0:STATUS_COL]\n",
    "y = dataset.iloc[:, [STATUS_COL]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_scorer(y_true, y_pred):\n",
    "    '''Scorer of Area Under Curve value'''\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, y_pred)\n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search 1st to apporach the best solution, GridSearch to refine it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDecisionTreeBest(X, y, debug=True):\n",
    "    '''Get the Decision Tree Hyper Parameters'''\n",
    "\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in range(2, 20, 4)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = {'criterion': ['gini', 'entropy'],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split':  [2, 5, 10],\n",
    "            'min_samples_leaf':  [1, 2, 4]}\n",
    "    \n",
    "    if debug:\n",
    "        pp.pprint(grid)\n",
    "    \n",
    "    # Using the grid search for best hyperparameters\n",
    "    dt = create_DT()\n",
    "    dt_grid = GridSearchCV(estimator = dt,\n",
    "                           param_grid = grid,\n",
    "                           scoring=metrics.make_scorer(auc_scorer,\n",
    "                                                       greater_is_better=True),\n",
    "                           cv=K_FOLD_NUM_SPLITS,\n",
    "                           verbose=2,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    dt_grid = dt_grid.fit(X, y)\n",
    "    \n",
    "    if debug:\n",
    "        print('Best Score: ', dt_grid.best_score_)\n",
    "        print('Best Params: ', dt_grid.best_params_)\n",
    "        \n",
    "    # Return score, method & params tuple\n",
    "    return (dt_grid.best_score_, 'Decision Tree', dt_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomForestBest(X, y, debug=True):\n",
    "    '''Get the Random Forest Hyper Parameters'''\n",
    "\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in range(2, 20, 4)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = {'n_estimators': [int(x) for x in range(2, 20, 2)],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'max_depth': max_depth,\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'min_samples_split':  [2, 5, 10],\n",
    "            'min_samples_leaf':  [1, 2, 4],\n",
    "            'bootstrap': [True, False]}\n",
    "    \n",
    "    if debug:\n",
    "        pp.pprint(grid)\n",
    "    \n",
    "    # Using the grid search for best hyperparameters\n",
    "    rf = create_RF()\n",
    "    rf_grid = GridSearchCV(estimator = rf,\n",
    "                           param_grid = grid,\n",
    "                           scoring=metrics.make_scorer(auc_scorer,\n",
    "                                                       greater_is_better=True),\n",
    "                           cv=K_FOLD_NUM_SPLITS,\n",
    "                           verbose=2,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    rf_grid = rf_grid.fit(X, y)\n",
    "    \n",
    "    if debug:\n",
    "        print('Best Score: ', rf_grid.best_score_)\n",
    "        print('Best Params: ', rf_grid.best_params_)\n",
    "        \n",
    "    # Return score, method & params tuple\n",
    "    return (rf_grid.best_score_, 'Random Forest', rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradientBoostBest(X, y, debug=True):\n",
    "    '''Get the Gradient Boost Hyper Parameters'''\n",
    "\n",
    "    # Create the grid parameters\n",
    "    grid = {'n_estimators': [int(x) for x in range(2, 20, 2)],\n",
    "            'learning_rate': [0.1, 0.3, 0.5, 0.7],\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            'criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "            'min_samples_split':  [2, 5, 10],\n",
    "            'min_samples_leaf':  [1, 2, 4],\n",
    "            'random_state': [SEED]}\n",
    "    \n",
    "    if debug:\n",
    "        pp.pprint(grid)\n",
    "    \n",
    "    # Using the grid search for best hyperparameters\n",
    "    gb = create_GB()\n",
    "    gb_grid = GridSearchCV(estimator = gb,\n",
    "                           param_grid = grid,\n",
    "                           scoring=metrics.make_scorer(auc_scorer,\n",
    "                                                       greater_is_better=True),\n",
    "                           cv=K_FOLD_NUM_SPLITS,\n",
    "                           verbose=2,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    gb_grid = gb_grid.fit(X, y)\n",
    "    \n",
    "    if debug:\n",
    "        print('Best Score: ', gb_grid.best_score_)\n",
    "        print('Best Params: ', gb_grid.best_params_)\n",
    "        \n",
    "    # Return score, method & params tuple\n",
    "    return (gb_grid.best_score_, 'Gradient Boosting', gb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the best algorithm\n",
    "algorithms = [getDecisionTreeBest(X, y),\n",
    "              getGradientBoostBest(X, y),\n",
    "              getRandomForestBest(X, y)]\\\n",
    "             .sort(reverse=True, key=lambda el: el[0])\n",
    "\n",
    "for entry, index in algorithms:\n",
    "    print('%i. %s - %f' % (index, entry[1], entry[0]))\n",
    "    \n",
    "print('Best algorithm: %s\\n\\tConfig:\\n' % algorithms[0][1])\n",
    "pprint(algorithms[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using method with higher score with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation settings\n",
    "auc_scores = []\n",
    "confusion_matrixes = []\n",
    "cv = KFold(n_splits=K_FOLD_NUM_SPLITS, random_state=SEED, shuffle=False)\n",
    "\n",
    "# CHANGE THIS LINE TO CHANGE THE USED CLASSIFICATION METHOD\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=2, min_samples_split=10, splitter='best')\n",
    "\n",
    "#classifier = RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=10, n_estimators=10)\n",
    "\n",
    "# Applying Cross validation\n",
    "for train_index, test_index in cv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Training with this fold\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Testing & Measuring accuracy\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "    auc_scores.append(metrics.auc(fpr, tpr))\n",
    "    confusion_matrixes.append(metrics.confusion_matrix(y_test, y_pred).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the obtained results\n",
    "print('Classification Method used:', classifier, '\\n')\n",
    "print('AUC scores:', auc_scores)\n",
    "print('> Average: ', sum(auc_scores)/len(auc_scores))\n",
    "for cf in confusion_matrixes:\n",
    "    display_confusion_matrix(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After having our model trained we shall use the model on the data to be sumitted in the kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset =  utils.read_csv_to_df('dataset/test_dataset.csv')\n",
    "ids = utils.read_csv_to_df('dataset/ids.csv')\n",
    "display(test_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now remove the Y column with NaNs\n",
    "test_dataset = test_dataset.iloc[:, 0:STATUS_COL]\n",
    "display(test_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model to get the 'status' predictions\n",
    "display(test_dataset)\n",
    "predictions_df = test_dataset.copy()\n",
    "predictions_df['Predicted'] = classifier.predict(test_dataset)\n",
    "predictions_df = ids.merge(predictions_df, on=['date', 'amount'])\n",
    "predictions_df = predictions_df[['loan_id', 'Predicted']]\\\n",
    "                    .rename(columns={\n",
    "                        'loan_id': 'Id'\n",
    "                    })\\\n",
    "                    .drop_duplicates()\n",
    "\n",
    "display(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting predictions to .csv\n",
    "# CHANGE FILE NAME TO PRESERVE DIFFERENT INSTANCES\n",
    "utils.write_df_to_csv(predictions_df, 'predictions', 'prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
