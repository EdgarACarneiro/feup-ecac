{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary libraries for prediciton\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_matrix(matrix, row_label, col_label):\n",
    "    \"\"\"Pretty print of the given matrix \"\"\"\n",
    "\n",
    "    # Restraining labels that are too big\n",
    "    row_label = [el[:10] + '..' if len(el) > 10 else el\n",
    "                for el in row_label]\n",
    "    col_label = [el[:10] + '..' if len(el) > 10 else el\n",
    "                for el in col_label]\n",
    "\n",
    "    # Stringfying everything & Joining top label\n",
    "    s_matrix = [list([\" \"] + (col_label))] + \\\n",
    "               [[row_label[row_idx]] + \\\n",
    "                [str(e) for e in row] for row_idx, row in enumerate(matrix)]\n",
    "\n",
    "    # Length of each matrix column\n",
    "    len_s = [max(map(len, col)) for col in zip(*s_matrix)]\n",
    "\n",
    "    # Cell formatation\n",
    "    formatation = '\\t'.join('{{:{}}}'.format(x) for x in len_s)\n",
    "\n",
    "    # Apply cell formation to each matrix element\n",
    "    pretty_mat = [formatation.format(*row) for row in s_matrix]\n",
    "\n",
    "    # Print Pretty Matrix\n",
    "    print('\\n'.join(pretty_mat))\n",
    "\n",
    "\n",
    "def display_confusion_matrix(values):\n",
    "    '''Display the given array as a confusion matrix'''\n",
    "    pretty_matrix([values[0:2], values[2:4]],\n",
    "                  ['Actual NO', 'Actual YES'],\n",
    "                  ['Predic NO', 'Predic YES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(fpr, tpr, roc_auc):\n",
    "    '''Plot the ROC-AUC curve'''\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'w--')\n",
    "    plt.xlim([-0.02, 1.02])\n",
    "    plt.ylim([0, 1])\n",
    "    ax.fill_between(fpr, 0, tpr)\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(df, columns):\n",
    "    '''Normalize the given columns for range between [0, 1]'''\n",
    "    for col in columns:\n",
    "        col_min = df[col].min()\n",
    "        col_max =  df[col].max()\n",
    "        \n",
    "        df[col] = (df[col] - col_min)/\\\n",
    "                    (col_max - col_min)\n",
    "        \n",
    "def normalize_df(df):\n",
    "    '''Normalize all columns of the given df'''\n",
    "    normalize_columns(df, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_PCA(df, variance_val=0.95, debug=True):\n",
    "    '''Apply the PCA algorithm to given dataframe,\n",
    "    using the given variance val to trim the df'''\n",
    "    # Necessary to normalize all data to use PCA\n",
    "    scaler=StandardScaler()\n",
    "    X_scaled=scaler.fit_transform(df)\n",
    "\n",
    "    # PCA - keep, by default mode, 90% variance\n",
    "    pca = PCA(variance_val)    \n",
    "    pca.fit(X_scaled)\n",
    "    X_pca = pca.transform(X_scaled)\n",
    "\n",
    "    if debug:\n",
    "        ex_variance=np.var(X_pca,axis=0)\n",
    "        ex_variance_ratio = ex_variance/np.sum(ex_variance)\n",
    "        print(' > Impact in total variance of each generated feature by PCA:')\n",
    "        print(ex_variance_ratio)\n",
    "\n",
    "    principal_df = pd.DataFrame(data = X_pca, index = df.reset_index()['loan_id'])\n",
    "    \n",
    "    return (principal_df, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_scorer(y_true, y_pred):\n",
    "    '''Scorer of Area Under Curve value'''\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, y_pred)\n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Algorithms\n",
    "\n",
    "* Logistic Regression\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LR():\n",
    "    '''Create a Logistic Regression model'''\n",
    "    return LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DT():\n",
    "    '''Create a new Decision Tree'''\n",
    "    # Useful DecisionTree tutorial:\n",
    "    # https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "    return DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RF():\n",
    "    '''Create a new Ranfom Forest model'''\n",
    "    return RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_GB():\n",
    "    '''Create a new Gradient Boosting model'''\n",
    "    return GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "* Predictions are done in this notebook.\n",
    "* It is also useful to compare how serveral algorithms perform against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# Useful Macros\n",
    "K_FOLD_NUM_SPLITS = 5\n",
    "SEED = 42\n",
    "USE_PCA = False\n",
    "UNDERSAMPLE = False\n",
    "UNDERSAMPLE_RATIO = 0.3\n",
    "\n",
    "# Pretty printer\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  utils.read_csv_to_df('dataset/preprocessed_data.csv')\n",
    "\n",
    "if UNDERSAMPLE:\n",
    "    print(' > Apllying undersampling:')\n",
    "    entries_df = len(dataset.index)\n",
    "\n",
    "    # Getting all minor class cases into final dataset\n",
    "    minor_df = dataset[dataset['status'] == -1]\n",
    "    num_minor = len(minor_df.index)\n",
    "    \n",
    "    print('\\t> Classes initial ratio: %f - %f\\n\\t> Dataset size: %i' %\n",
    "         (num_minor / entries_df, (entries_df - num_minor) / entries_df, entries_df))\n",
    "\n",
    "    # Selecting equal number from major class\n",
    "    major_df = dataset[dataset['status'] == 1].sample(n=int((num_minor / UNDERSAMPLE_RATIO) - num_minor),\n",
    "                                                      random_state=SEED)\n",
    "    num_major = len(major_df.index)\n",
    "    total_under = num_minor + num_major\n",
    "    \n",
    "    print('\\t> Classes final ratio: %f - %f\\n\\t> Dataset size: %i' % \n",
    "          (num_minor / total_under, num_major / total_under, total_under))\n",
    "\n",
    "    # Concatenting to main dataframe\n",
    "    dataset = pd.concat([minor_df, major_df])\n",
    "\n",
    "dataset = dataset.set_index('loan_id')\n",
    "display(dataset)\n",
    "\n",
    "# Normalizing dataset\n",
    "print(' > Dataset after normalization')\n",
    "normalize_df(dataset)\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_COL = dataset.columns.get_loc(\"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and Y\n",
    "X = dataset.iloc[:, 0:STATUS_COL]\n",
    "y = dataset.iloc[:, [STATUS_COL]]\n",
    "display(X.head())\n",
    "\n",
    "if USE_PCA:\n",
    "    print(' > Applying PCA to X_train:')\n",
    "    X, pca = apply_PCA(X, debug=True)\n",
    "    display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search 1st to apporach the best solution, GridSearch to refine it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLogisticRegressionBest(X, y, debug=True):\n",
    "    '''Get the Logistic Regression Hyper Parameters'''\n",
    "\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in range(2, 20, 4)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = {'penalty': ['l2', 'none'],\n",
    "            'C': [0.01, 0.05, 0.1, 0.2, 0.5, 1],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "            'class_weight': [\"balanced\", None]}\n",
    "    \n",
    "    if debug:\n",
    "        pp.pprint(grid)\n",
    "    \n",
    "    # Using the grid search for best hyperparameters\n",
    "    lr = create_LR()\n",
    "    lr_grid = GridSearchCV(estimator = lr,\n",
    "                           param_grid = grid,\n",
    "                           scoring=metrics.make_scorer(auc_scorer,\n",
    "                                                       greater_is_better=True),\n",
    "                           cv=K_FOLD_NUM_SPLITS,\n",
    "                           verbose=2,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    lr_grid = lr_grid.fit(X, y)\n",
    "    \n",
    "    if debug:\n",
    "        print('Best Score: ', lr_grid.best_score_)\n",
    "        print('Best Params: ', lr_grid.best_params_)\n",
    "        \n",
    "    # Return score, method & params tuple\n",
    "    return (lr_grid.best_score_, 'Logistic Regression', lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDecisionTreeBest(X, y, debug=True):\n",
    "    '''Get the Decision Tree Hyper Parameters'''\n",
    "\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in range(2, 20, 4)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = {'criterion': ['gini', 'entropy'],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split':  [2, 5, 10],\n",
    "            'min_samples_leaf':  [1, 2, 4],\n",
    "            'class_weight': [\"balanced\", None]}\n",
    "    \n",
    "    if debug:\n",
    "        pp.pprint(grid)\n",
    "    \n",
    "    # Using the grid search for best hyperparameters\n",
    "    dt = create_DT()\n",
    "    dt_grid = GridSearchCV(estimator = dt,\n",
    "                           param_grid = grid,\n",
    "                           scoring=metrics.make_scorer(auc_scorer,\n",
    "                                                       greater_is_better=True),\n",
    "                           cv=K_FOLD_NUM_SPLITS,\n",
    "                           verbose=2,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    dt_grid = dt_grid.fit(X, y)\n",
    "    \n",
    "    if debug:\n",
    "        print('Best Score: ', dt_grid.best_score_)\n",
    "        print('Best Params: ', dt_grid.best_params_)\n",
    "        \n",
    "    # Return score, method & params tuple\n",
    "    return (dt_grid.best_score_, 'Decision Tree', dt_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomForestBest(X, y, debug=True):\n",
    "    '''Get the Random Forest Hyper Parameters'''\n",
    "\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in range(2, 16, 4)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = {'n_estimators': [int(x) for x in range(2, 14, 2)],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'max_depth': max_depth,\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'min_samples_split':  [2, 5, 10],\n",
    "            'min_samples_leaf':  [1, 2, 4],\n",
    "            'bootstrap': [True, False],\n",
    "            'class_weight': [\"balanced\", \"balanced_subsample\", None]}\n",
    "    \n",
    "    if debug:\n",
    "        pp.pprint(grid)\n",
    "    \n",
    "    # Using the grid search for best hyperparameters\n",
    "    rf = create_RF()\n",
    "    rf_grid = GridSearchCV(estimator = rf,\n",
    "                           param_grid = grid,\n",
    "                           scoring=metrics.make_scorer(auc_scorer,\n",
    "                                                       greater_is_better=True),\n",
    "                           cv=K_FOLD_NUM_SPLITS,\n",
    "                           verbose=2,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    rf_grid = rf_grid.fit(X, y)\n",
    "    \n",
    "    if debug:\n",
    "        print('Best Score: ', rf_grid.best_score_)\n",
    "        print('Best Params: ', rf_grid.best_params_)\n",
    "        \n",
    "    # Return score, method & params tuple\n",
    "    return (rf_grid.best_score_, 'Random Forest', rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradientBoostBest(X, y, debug=True):\n",
    "    '''Get the Gradient Boost Hyper Parameters'''\n",
    "\n",
    "    # Create the grid parameters\n",
    "    grid = {'n_estimators': [int(x) for x in range(2, 14, 2)],\n",
    "            'learning_rate': [0.1, 0.3, 0.5, 0.7],\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            'criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "            'min_samples_split':  [2, 5, 10],\n",
    "            'min_samples_leaf':  [1, 2, 4],\n",
    "            'random_state': [SEED]}\n",
    "    \n",
    "    if debug:\n",
    "        pp.pprint(grid)\n",
    "    \n",
    "    # Using the grid search for best hyperparameters\n",
    "    gb = create_GB()\n",
    "    gb_grid = GridSearchCV(estimator = gb,\n",
    "                           param_grid = grid,\n",
    "                           scoring=metrics.make_scorer(auc_scorer,\n",
    "                                                       greater_is_better=True),\n",
    "                           cv=K_FOLD_NUM_SPLITS,\n",
    "                           verbose=2,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    gb_grid = gb_grid.fit(X, y)\n",
    "    \n",
    "    if debug:\n",
    "        print('Best Score: ', gb_grid.best_score_)\n",
    "        print('Best Params: ', gb_grid.best_params_)\n",
    "        \n",
    "    # Return score, method & params tuple\n",
    "    return (gb_grid.best_score_, 'Gradient Boosting', gb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting the best algorithm\n",
    "algorithms = [getLogisticRegressionBest(X, y),\n",
    "              getDecisionTreeBest(X, y),\n",
    "              getRandomForestBest(X, y)]\n",
    "              # getGradientBoostBest(X, y)]\n",
    "algorithms.sort(reverse=True, key=lambda el: el[0])\n",
    "\n",
    "for index, entry in enumerate(algorithms):\n",
    "    print('%i. %s - %f\\n---------' % (index + 1, entry[1], entry[0]))\n",
    "    \n",
    "print('Best algorithm: %s' % algorithms[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using method with higher score with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation settings\n",
    "auc_scores = []\n",
    "confusion_matrixes = []\n",
    "cv = KFold(n_splits=K_FOLD_NUM_SPLITS, random_state=SEED, shuffle=False)\n",
    "\n",
    "# CHANGE THIS LINE TO CHANGE THE USED CLASSIFICATION METHOD\n",
    "# classifier = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=2, min_samples_split=10, splitter='best')\n",
    "# classifier = GradientBoostingClassifier(criterion='friedman_mse', learning_rate=0.7, loss='deviance',\n",
    "#                                         min_samples_leaf=2, min_samples_split=2, n_estimators=8, random_state=SEED)\n",
    "classifier = RandomForestClassifier(bootstrap=False, class_weight='balanced', criterion='entropy',\n",
    "                                    max_depth=2, max_features='auto', min_samples_leaf=4,\n",
    "                                    min_samples_split=2, n_estimators=8)\n",
    "\n",
    "\n",
    "# Applying Cross validation\n",
    "for train_index, test_index in cv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Training with this fold\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Testing & Measuring accuracy\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    auc_scores.append(auc)\n",
    "    confusion_matrixes.append(metrics.confusion_matrix(y_test, y_pred).ravel())\n",
    "\n",
    "    plot_roc_auc(fpr, tpr, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the obtained results\n",
    "print('Classification Method used:', classifier, '\\n')\n",
    "print('AUC scores:', auc_scores)\n",
    "print('> Average: ', sum(auc_scores)/len(auc_scores))\n",
    "for cf in confusion_matrixes:\n",
    "    display_confusion_matrix(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After having our model trained we shall use the model on the data to be sumitted in the kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset =  utils.read_csv_to_df('dataset/test_dataset.csv')\n",
    "test_dataset = test_dataset.set_index('loan_id')\n",
    "normalize_df(test_dataset)\n",
    "display(test_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now remove the Y column with NaNs\n",
    "test_dataset = test_dataset.iloc[:, 0:STATUS_COL]\n",
    "\n",
    "display(test_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "\n",
    "if USE_PCA:\n",
    "    # Using train PCA and classifying\n",
    "    scaler=StandardScaler()\n",
    "    X_test_scaled=scaler.fit_transform(test_dataset)\n",
    "    predictions_df = pd.DataFrame(data = pca.transform(X_test_scaled),\n",
    "                                  index=test_dataset.reset_index()['loan_id'])\n",
    "    display(predictions_df)\n",
    "\n",
    "    predictions_df['Predicted'] = classifier.predict(predictions_df)\n",
    "    final_df = predictions_df.reset_index()\\\n",
    "                    [['loan_id', 'Predicted']]\\\n",
    "                    .rename(columns={\n",
    "                        'loan_id': 'Id'\n",
    "                    })\n",
    "    \n",
    "else:\n",
    "    final_df = test_dataset.copy()\n",
    "    final_df['Predicted'] = classifier.predict(final_df)\n",
    "    final_df = final_df.reset_index()\\\n",
    "                        [['loan_id', 'Predicted']]\\\n",
    "                        .rename(columns={\n",
    "                            'loan_id': 'Id'\n",
    "                        })\\\n",
    "                        .drop_duplicates()\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting predictions to .csv\n",
    "# CHANGE FILE NAME TO PRESERVE DIFFERENT INSTANCES\n",
    "utils.write_df_to_csv(final_df, 'predictions', 'prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
